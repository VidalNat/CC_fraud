# -*- coding: utf-8 -*-
"""Credit Card Fraud

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bTKcIQZwV67sHD3fKK0JbUbYm8FDrFNv

##Importing libraries
"""

import numpy as np
import pandas as pd
from datetime import datetime
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/drive')

"""##Data processing"""

df = pd.read_csv('/content/drive/MyDrive/Credit-fraud project/fraudTrain.csv')

"""**Dataset columns** -

transdatetrans_time - 	Transaction DateTime

merchant	- Merchant Name

category	 -Category of Merchant

amt	 - Amount of Transaction

city	- City of Credit Card Holder

state	- State of Credit Card Holder

lat	- Latitude Location of Purchase

long	- Longitude Location of Purchase

city_pop	- Credit Card Holder's City Population

job	- Job of Credit Card Holder

dob	- Date of Birth of Credit Card Holder

trans_num	- Transaction Number

merch_lat	- Latitude Location of Merchant

merch_long	- Longitude Location of Merchant

is_fraud	- Whether Transaction is Fraud (1) or Not (0)
"""

df.columns

df.drop(['Unnamed: 0'],axis=1,inplace=True) #-- Removing unneccsary 'Unnamed' columns
# df = df.drop(df.columns[df.columns.str.contains('^Unnamed')],axis=1) -- optional

df.columns = [col.strip() for col in df.columns]

pd.options.display.float_format = '{:.2f}'.format # figures Pandas to display floating-point numbers in a format with two decimal places.
df[['amt']].describe()

df.dropna(inplace=True)

df.isna().sum()

df.duplicated().sum()

# Data preprocessing & feature engineering
df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'],format='%Y-%m-%d %H:%M:%S',errors='coerce')

df['dob'] = pd.to_datetime(df['dob'])
df['age'] = df['dob'].apply(lambda x: datetime.now().year - x.year - ((datetime.now().month, datetime.now().day) < (x.month, x.day)))

df['trans_month'] = df['trans_date_trans_time'].dt.month
df['trans_hour'] = df['trans_date_trans_time'].dt.hour
df['trans_day_of_week'] = df['trans_date_trans_time'].dt.dayofweek

from geopy.distance import geodesic
# Preprocessing data for visualization
# NumPy arrays of coordinates to be used in geodesic
coords_user = np.array(list(zip(df['lat'], df['long'])))
coords_merch = np.array(list(zip(df['merch_lat'], df['merch_long'])))

# Define a function to calculate distance for a pair of coordinates
def vectorized_geodesic(coord1, coord2):
    return np.array([geodesic(c1, c2).km for c1, c2 in zip(coord1, coord2)])

# Calculate distances
df['distances'] = vectorized_geodesic(coords_user, coords_merch)

"""##Analysing data"""

df['distances'].describe()

# Create distance bins (based on insights from describe() results
bins = [0, 20, 50, 90, 120, float('inf')]
labels = ['0-20', '20-50', '50-90', '90-120', '120+']

# Categorize distances into bins
df['distance_category'] = pd.cut(df['distances'], bins=bins, labels=labels, right=False)

fraud_by_distance = df.groupby('distance_category')['is_fraud'].count()
fraud_by_distance

fig = px.bar(
    fraud_by_distance,
    x=fraud_by_distance.index,
    y=fraud_by_distance.values,
    title='Transaction Count by Distance Category',
    labels={'distance_category': 'Distance Category (km)', 'count': 'Fraud Count'},
    text=fraud_by_distance.values,
    width=600,
    height=400
)

# Optional: Customizes the text to appear above the bar, matching the original more closely
fig.update_traces(texttemplate='%{text}', textposition='outside')
fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')

fig.show()

fig = px.scatter(
    df,
    x='amt',
    y='is_fraud',
    color='is_fraud',
    title='Relationship between Transaction Amount and Fraud',
    labels={'amt': 'Transaction Amount', 'is_fraud': 'Fraud (1: Yes, 0: No)'},
    # Set the colors to match the original seaborn palette: Blue for 0 (No Fraud), Red for 1 (Fraud)
    color_discrete_map={'0': 'blue', '1': 'red'},
    width=700, # Set size as requested in previous turn
    height=400 # Set size as requested in previous turn
)

# Optional: Adjust the y-axis for better readability and explicit labels
fig.update_layout(
    yaxis=dict(
        tickmode='array',
        tickvals=['0', '1'],
        ticktext=['No (0)', 'Yes (1)'],
        title='Fraud (1: Yes, 0: No)'
    ),
    xaxis_title='Transaction Amount'
)

fig.show()

# Exploring fraud on categories
fraud_rate_by_category = df.groupby('category')['is_fraud'].mean().sort_values(ascending=False)

# Apply custom formatting to the Series
fraud_rate_by_category = fraud_rate_by_category.map('{:.2%}'.format)

print(fraud_rate_by_category)

"""##Random forest model"""

# Processing for random forest
#df.to_csv('preprocessed_data.csv', index=False)
preprocessed_data = df.dropna()#making an duplicate of processed data

# Column processing
categorical_columns = ['category', 'gender']

# One-Hot Encoding (for nominal features)
from pandas import get_dummies
preprocessed_data = pd.get_dummies(preprocessed_data, columns=categorical_columns, drop_first=True)

# Prepare Feature Matrix (X) and Target Vector (y)
X = preprocessed_data.drop(columns=['trans_date_trans_time', 'cc_num', 'merchant',
       'first', 'last', 'lat','city','state',
       'long','job', 'dob','street', 'trans_num', 'unix_time', 'merch_lat',
       'merch_long','distance_category', 'is_fraud'], axis=1) # Ensure 'is_fraud' is dropped from features
y = preprocessed_data['is_fraud']

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Fit Random Forest Model
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Evaluate
accuracy = rf.score(X_test, y_test)
print(f"Random Forest Model Accuracy: {accuracy:.2f}")

# Adjust threshold for overall good recall value as 1.00 accuracy might be flawed

from sklearn.metrics import precision_score, recall_score, f1_score

thresholds = np.arange(0.1, 0.9, 0.1)
for t in thresholds:
    y_pred_adj = (rf.predict_proba(X_test)[:, 1] > t).astype(int)
    precision = precision_score(y_test, y_pred_adj)
    recall = recall_score(y_test, y_pred_adj)
    f1 = f1_score(y_test, y_pred_adj)
    print(f"Threshold: {t:.1f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-Score: {f1:.2f}")

"""##grid-search (optional)"""

# # Grid-search
# param_grid = {
#        'n_estimators': [100, 200, 300],  # Number of trees in the forest
#        'max_depth': [None, 10, 20],  # Maximum depth of the trees
#        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
#        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
#    }

# from sklearn.model_selection import GridSearchCV
# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, scoring='recall', cv=5)  # cv is the number of cross-validation folds

# grid_search.fit(X_train, y_train)
# best_model = grid_search.best_estimator_

"""### Evaluating Thresholds on New Test Data"""

from sklearn.metrics import precision_score, recall_score, f1_score
import numpy as np

# Assuming 'probs' contains the predicted probabilities for the new test data
# Assuming 'df_test['is_fraud']' contains the actual labels for the new test data

thresholds = np.arange(0.1, 1.0, 0.1) # Include 1.0 to see extreme case

print("Threshold Evaluation on New Test Data:")
print("-" * 50)

for t in thresholds:
    # Apply threshold to get predicted labels
    y_pred_new_threshold = (probs > t).astype(int)

    # Calculate metrics
    precision = precision_score(df_test['is_fraud'], y_pred_new_threshold)
    recall = recall_score(df_test['is_fraud'], y_pred_new_threshold)
    f1 = f1_score(df_test['is_fraud'], y_pred_new_threshold)

    print(f"Threshold: {t:.1f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}")

print("-" * 50)

# Get predicted probabilities
y_probs = rf.predict_proba(X_test)[:, 1]  # Probabilities of the positive class

"""1. If detecting fraud (minimizing FN) is critical,  threshold can be of  0.2.

2. If reducing false positives (minimizing FP) is more important, threshold can be of 0.25.

**As per general use case:**

1. High Risk/High Cost of Fraud (e.g., banking, e-commerce):
* Lower threshold (e.g., 0.2) to maximize fraud detection (recall).
* Combine the model with a manual review process for flagged transactions to reduce FP impact.

2. Customer-Centric Scenarios (e.g., retail, subscriptions):
* Higher threshold (e.g., 0.25 to 0.3) to minimize disruptions.
* Use an anomaly detection system alongside to monitor high-risk transactions.

3. Hybrid Approach:
* Use a lower threshold initially (e.g., 0.2) to cast a wide net.
* Implement tiered flagging rules:
* Transactions with probabilities closer to the threshold (e.g., 0.2â€“0.3) undergo manual review.
* Transactions above a stricter threshold (e.g., >0.4) are automatically blocked or flagged.
"""

# Apply your overall balanced threshold which is 0.25
threshold = 0.25
y_pred = (y_probs > threshold).astype(int)

X_test['is_fraud_pred'] = y_pred

X_test['is_fraud'] = y_test

X_test[(X_test['is_fraud'] == 1)&(X_test['is_fraud_pred'] == 0)][['is_fraud_pred',	'is_fraud']].count()

X_test[(X_test['is_fraud'] == 0)&(X_test['is_fraud_pred'] == 1)][['is_fraud_pred',	'is_fraud']].count()

"""##save the model(optional)

##Test & evaluations
"""

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

from sklearn.metrics import confusion_matrix

conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

roc_auc = roc_auc_score(y_test, y_probs)
print(f"ROC-AUC Score: {roc_auc:.2f}")


# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs)
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.2f})")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend()
plt.show()

# Get Feature Importances
importances = rf.feature_importances_
features = X_train.columns

# Create a DataFrame for Visualization
feature_importances = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Plot the Feature Importances
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')
plt.gca().invert_yaxis()
plt.title("Feature Importances")
plt.xlabel("Importance")
plt.show()

# Select top 10 features
top_indices = np.argsort(importances)[-10:]
top_features = features[top_indices]
top_importances = importances[top_indices]

# Create horizontal bar plot
plt.figure(figsize=(10, 6))
plt.barh(top_features, top_importances)
plt.xlabel("Feature Importance")
plt.ylabel("Feature Name")
plt.title("Top 10 Feature Importances")
plt.xticks(rotation=45, ha='right')
plt.show()

"""High Accuracy: Indicates the model is performing well overall, but for imbalanced datasets, focus more on precision, recall, and F1-score.

Precision vs. Recall: For fraud detection, a higher recall (identifying more actual frauds) is often more critical, even if precision is slightly sacrificed.

ROC-AUC > 0.8: Indicates a good level of discrimination between fraud and non-fraud.
"""

import joblib
# Assuming 'rf' is your trained Random Forest model
joblib.dump(rf, 'random_forest_model.pkl')
loaded_rf = joblib.load('random_forest_model.pkl')

"""## Applying the RF model on newly processed test data"""

df_test = pd.read_csv('/content/drive/MyDrive/Credit-fraud project/fraudTest.csv')
df_test.drop(['Unnamed: 0'],axis=1,inplace=True)
df_test.columns = [col.strip() for col in df_test.columns]
pd.options.display.float_format = '{:.2f}'.format # Cnfigures Pandas to display floating-point numbers in a format with two decimal places.
df_test[['amt']].describe()
df_test.dropna(inplace=True)
df_test.drop_duplicates(inplace=True)

# Data preprocessing & feature engineering

df_test['trans_date_trans_time'] = pd.to_datetime(df_test['trans_date_trans_time'],format='%Y-%m-%d %H:%M:%S',errors='coerce')

df_test['dob'] = pd.to_datetime(df_test['dob'])

df_test['age'] = df_test['dob'].apply(lambda x: datetime.now().year - x.year - ((datetime.now().month, datetime.now().day) < (x.month, x.day)))

df_test['trans_month'] = df_test['trans_date_trans_time'].dt.month

df_test['trans_hour'] = df_test['trans_date_trans_time'].dt.hour

df_test['trans_day_of_week'] = df_test['trans_date_trans_time'].dt.dayofweek

from geopy.distance import geodesic

# Create NumPy arrays of coordinates
coords_user = np.array(list(zip(df_test['lat'], df_test['long'])))
coords_merch = np.array(list(zip(df_test['merch_lat'], df_test['merch_long'])))

# Define a function to calculate distance for a pair of coordinates

def vectorized_geodesic(coord1, coord2):
  return np.array([geodesic(c1, c2).km for c1, c2 in zip(coord1, coord2)])

# Calculate distances
df_test['distances'] = vectorized_geodesic(coords_user, coords_merch)

# Column processing
test_categorical_columns = ['category', 'gender']

# One-Hot Encoding (for nominal features)
from pandas import get_dummies
df_test = pd.get_dummies(df_test, columns=test_categorical_columns, drop_first=True)

training_feature_columns = df_test.drop(columns=['trans_date_trans_time', 'cc_num', 'merchant',
       'first', 'last', 'lat','city','state',
       'long','job', 'dob','street', 'trans_num', 'unix_time', 'merch_lat',
       'merch_long','is_fraud'])

X_test_new = training_feature_columns  # Use the same columns as in training

# Get predicted probabilities using the loaded model
probs = loaded_rf.predict_proba(X_test_new_reindexed)[:, 1]

# Apply the chosen threshold (e.g., 0.2 for better recall)
threshold = 0.2
predictions = (probs > threshold).astype(int)

# Add predictions to the test dataset
df_test['is_fraud_pred'] = predictions

print(df_test['is_fraud'].isnull().sum())  # Assuming 'is_fraud' is your target column

df_test = df_test.dropna(subset=['is_fraud'])

df_test2 = pd.read_csv('/content/drive/MyDrive/Credit-fraud project/fraudTest.csv')

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the confusion matrix
conf_matrix_0_2 = confusion_matrix(df_test['is_fraud'], df_test['is_fraud_pred'])

# Create labels for the matrix
labels = ['Not Fraud (0)', 'Fraud (1)']

# Visualize the confusion matrix using Seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_0_2, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for New Test Data (Threshold 0.2)')
plt.show()

# Calculate and print Precision, Recall, and F1-score
precision_0_2 = precision_score(df_test['is_fraud'], df_test['is_fraud_pred'])
recall_0_2 = recall_score(df_test['is_fraud'], df_test['is_fraud_pred'])
f1_0_2 = f1_score(df_test['is_fraud'], df_test['is_fraud_pred'])

print("\nPerformance Metrics on New Test Data (Threshold 0.2):")
print(f"Precision: {precision_0_2:.4f}")
print(f"Recall: {recall_0_2:.4f}")
print(f"F1-Score: {f1_0_2:.4f}")